{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_pre.weight\n",
      "norm_pre.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt_path = \"/mnt/sda1/tkerssies/CoCa-ViT-B-32-laion2B-s13B-b90k.bin\"\n",
    "# ckpt_path = \"/mnt/sda1/tkerssies/Model-B-16_Data-2B_Samples-13B_lr-1e-3_bs-88k.pt\"\n",
    "\n",
    "ckpt = torch.load(ckpt_path)\n",
    "if \"state_dict\" in ckpt:\n",
    "    ckpt = ckpt[\"state_dict\"]\n",
    "\n",
    "ckpt = {k.replace(\"module.\", \"\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k: v for k, v in ckpt.items() if k.startswith(\"visual.\")}\n",
    "ckpt = {k.replace(\"visual.\", \"\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"transformer.\", \"\"): v for k, v in ckpt.items()}\n",
    "if \"proj\" in ckpt:\n",
    "    ckpt.pop(\"proj\")\n",
    "ckpt[\"cls_token\"] = ckpt.pop(\"class_embedding\")[None, None, :]\n",
    "ckpt[\"pos_embed\"] = ckpt.pop(\"positional_embedding\")[None, :]\n",
    "ckpt[\"patch_embed.proj.weight\"] = ckpt.pop(\"conv1.weight\")\n",
    "ckpt = {k.replace(\"ln_pre.\", \"norm_pre.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"resblocks.\", \"blocks.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"ln_1.\", \"norm1.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"ln_2.\", \"norm2.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"in_proj_weight\", \"qkv.weight\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"in_proj_bias\", \"qkv.bias\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"out_proj.weight\", \"proj.weight\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"out_proj.bias\", \"proj.bias\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"c_fc.\", \"fc1.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace(\"c_proj.\", \"fc2.\"): v for k, v in ckpt.items()}\n",
    "ckpt = {k: v for k, v in ckpt.items() if not k.startswith(\"attn_pool.\")}\n",
    "\n",
    "if ckpt[\"ln_post.weight\"].shape == ckpt[\"norm_pre.weight\"].shape:\n",
    "    ckpt = {k.replace(\"ln_post.\", \"norm.\"): v for k, v in ckpt.items()}\n",
    "else:\n",
    "    ckpt = {k: v for k, v in ckpt.items() if not k.startswith(\"ln_post.\")}\n",
    "    ckpt[\"norm.weight\"] = torch.ones_like(ckpt[\"norm_pre.weight\"])\n",
    "    ckpt[\"norm.bias\"] = torch.zeros_like(ckpt[\"norm_pre.bias\"])\n",
    "\n",
    "print(\"\\n\".join(ckpt.keys()))\n",
    "\n",
    "torch.save(ckpt, ckpt_path + \".timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-generalization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
