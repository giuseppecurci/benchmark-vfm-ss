{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemis/miniconda3/envs/benchmark-vfm-ss/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.linear_decoder import LinearDecoder\n",
    "from transformers import AutoModelForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = ['vit_large_patch14_dinov2', \"eva02_base_patch14_224\", \"depth-anything/Depth-Anything-V2-Base-hf\"]\n",
    "img_size = (1024, 1024) # img size 512 works with 8 and 16 patch size, not 14\n",
    "patch_size = 16\n",
    "num_classes = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemis/miniconda3/envs/benchmark-vfm-ss/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/artemis/miniconda3/envs/benchmark-vfm-ss/lib/python3.10/site-packages/torch/_functorch/deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "model = LinearDecoder(\n",
    "    encoder_name[0],\n",
    "    num_classes = num_classes,\n",
    "    img_size = img_size,\n",
    "    ckpt_path = None,\n",
    "    sub_norm = False, \n",
    "    patch_size = patch_size,\n",
    "    pretrained = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model.cuda()\n",
    "data = torch.randn(1, 3, *(img_size)).to(device)\n",
    "target = torch.randint(0, num_classes, (1, 1024, 1024)).to(device)\n",
    "dataset = torch.utils.data.TensorDataset(data, target)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "critertion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(data)\n",
    "segmentation = F.interpolate(logits, img_size, mode=\"bilinear\")\n",
    "loss = critertion(segmentation, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwarding Linear Decoder\n",
      " Called encoder forward\n",
      " Normalized: torch.Size([1, 3, 1024, 1024])\n",
      "     Forward features HF_models\n",
      "     Returning last layer: torch.Size([1, 5330, 768])\n",
      " Logits: torch.Size([1, 5330, 768])\n",
      "Got output from decoder: torch.Size([1, 5329, 768])\n",
      "Got output from head: torch.Size([1, 5329, 19])\n",
      "Transposed: torch.Size([1, 19, 5329])\n",
      "hw logits: torch.Size([1, 19, 73, 73])\n",
      "HW segmentation logits: torch.Size([1, 19, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(data)\n",
    "    print('hw logits:',logits.shape)\n",
    "    segmentation = F.interpolate(logits, img_size, mode=\"bilinear\")\n",
    "    print('HW segmentation logits:',segmentation.shape)\n",
    "    loss = critertion(segmentation, target)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemis/miniconda3/envs/benchmark-vfm-ss/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "depthv2 = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\",\n",
    "                                                    state_dict=None)\n",
    "depthv2.cuda()\n",
    "with torch.no_grad():\n",
    "    out = depthv2.backbone.embeddings(data)\n",
    "    out = depthv2.backbone.encoder(out)\n",
    "    out_back = depthv2.backbone(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DINOv2 (done)\n",
    "    - python main.py fit -c configs/cityscapes_linear_semantic.yaml --root /media/data/workspace_Giuseppe/code/datasets/CityScapes --data.num_workers 8 --trainer.devices [0,1] --model.network.encoder_name vit_large_patch14_dinov2 --model.network.patch_size 8\n",
    "\n",
    "- EVA02 \n",
    "    - 'at the moment OOM' on 24 GB, maybe works on 48 GBs\n",
    "    - patch size 16 and 1024 works, according to benchmark size 16 correlates with 8 so at least for a first experiment\n",
    "      it should be fine\n",
    "    - python main.py fit -c configs/cityscapes_linear_semantic.yaml --root /media/data/workspace_Giuseppe/code/datasets/CityScapes --data.num_workers 8 --trainer.devices [0,1] --model.network.encoder_name eva02_large_patch14_224.mim_m38m --no_compile --model.network.patch_size 8\n",
    "\n",
    "- Depth-v2 \n",
    "    - the memory consumption when using lightning seems higher, which is strange bc using mixed-precision 16 \n",
    "      and the training step consists of calling the code present in the notebook (+ just a couple more of computations)\n",
    "    - patch size 8 (even with torch.no_grad())/14 -> OOM\n",
    "    - python main.py fit -c configs/cityscapes_linear_semantic.yaml --root /media/data/workspace_Giuseppe/code/datasets/CityScapes --data.num_workers 8 --trainer.devices [0,1] --model.network.encoder_name depth-anything/Depth-Anything-V2-Base-hf --model.network.patch_size 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## DepthAny-V2 SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemis/miniconda3/envs/benchmark-vfm-ss/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "depthv2 = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\",\n",
    "                                                    state_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 3, 1024, 1024)\n",
    "\n",
    "model.cuda()\n",
    "depthv2.cuda()\n",
    "data = data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:\n",
      "torch.Size([1, 16385, 1024])\n",
      "torch.Size([1, 16384, 1024])\n",
      "linear decoder:\n",
      "torch.Size([1, 16384, 19])\n",
      "torch.Size([1, 19, 16384])\n",
      "torch.Size([1, 19, 128, 128])\n",
      "upsample:\n",
      "torch.Size([1, 19, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Dinov2 + Linear Decoder\n",
    "with torch.no_grad():\n",
    "    print('encoder:')\n",
    "    data = (data - model.pixel_mean) / model.pixel_std\n",
    "    output = model.encoder.forward_features(data)\n",
    "    print(output.shape)\n",
    "    if output.dim() == 4:\n",
    "        output = output.flatten(2).transpose(1, 2) \n",
    "    else: \n",
    "        # remove cls token\n",
    "        output = output[:, model.encoder.num_prefix_tokens :]\n",
    "    print(output.shape)\n",
    "    print('linear decoder:')\n",
    "    output = model.head(output)    \n",
    "    print(output.shape)\n",
    "    output = output.transpose(1,2)\n",
    "    print(output.shape)\n",
    "    logits = output.reshape(output.shape[0], -1, *model.grid_size)\n",
    "    print(logits.shape)\n",
    "    print('upsample:')\n",
    "    logits = F.interpolate(logits, (1024,1024), mode=\"bilinear\")\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384, 1024])\n",
      "torch.Size([1, 16385, 1024])\n",
      "torch.Size([1, 16385, 1024])\n",
      "torch.Size([1, 16385, 1024])\n",
      "torch.Size([1, 16385, 1024])\n"
     ]
    }
   ],
   "source": [
    "# DINOv2 encoder forward features\n",
    "with torch.no_grad():\n",
    "    out = model.encoder.patch_embed(data) # proj + flatten \n",
    "    print(out.shape)\n",
    "    out = model.encoder._pos_embed(out) # add positional embedding and cls token\n",
    "    print(out.shape)\n",
    "    out = model.encoder.patch_drop(out) # dropout\n",
    "    print(out.shape)\n",
    "    out = model.encoder.norm_pre(out) # layer norm\n",
    "    print(out.shape)\n",
    "    out_blocks = model.encoder.blocks(out) # transformer blocks\n",
    "    print(out_blocks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- paper \n",
    "    - depth: dinov2 encoder + DPT decoder \n",
    "    - SS: they say on top of our encoders, so I guess they only Dinov2 encoders? (Probably yes)\n",
    "        - is the forward pass modified? bc normally they only extract feats at 4 blocks' levels\n",
    "        - should I simply leave the forward as it is, reshape features out backbone and then apply linear decoder? \n",
    "          DinoV2 only final block features \n",
    "- create two versions (keep linear and same fine-tuning settings)\n",
    "    - use hf model backbone and discard all but last feature map (stage 24 = last block)\n",
    "        - test 14x14 patch size for less epochs\n",
    "        - if worked, then use 8x8 patch size\n",
    "    - use hf model backbone and modify forward pass to stack the feature maps, reshape and adjust linear decoder \n",
    "    - try the same, but change pre-processing to depthv2 pipeline\n",
    "\n",
    "- problems: \n",
    "    - neck outputs ms features \n",
    "        - pick only the largest one\n",
    "        - find way to combine\n",
    "        - NOTE: \"transferring our Depth Anything encoders to semantic segmentation\", does it mean I only use backbone and no neck? (lose ~30M params -> basically same params of DinoV2)\n",
    "            - stack backbone features and reshape to 3 dims\n",
    "            - take only the last one\n",
    "    - in paper use Mask2Former, not linear probe\n",
    "        - benchmark paper shows what ViT-g + mask2former does really well only for mIoU, but shit ECE, FPR@95 etc. for semantics, but excels in OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone dinov2:\n",
      "torch.Size([4, 1, 5330, 1024])\n",
      "neck:\n",
      "torch.Size([1, 256, 73, 73])\n",
      "torch.Size([1, 256, 146, 146])\n",
      "torch.Size([1, 256, 292, 292])\n",
      "torch.Size([1, 256, 584, 584])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('backbone dinov2:')\n",
    "    output_backbone = depthv2.backbone(data)\n",
    "    feat_maps_backbone = output_backbone.feature_maps\n",
    "    print(torch.stack(feat_maps_backbone).shape)\n",
    "\n",
    "_, _, height, width = data.shape\n",
    "patch_size = depthv2.config.patch_size\n",
    "patch_height = height // patch_size\n",
    "patch_width = width // patch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('neck:')\n",
    "    output_neck = depthv2.neck(feat_maps_backbone, patch_height, patch_width) # reassemble stage + fusion stage\n",
    "    for i in output_neck:\n",
    "        print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21320, 1024])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacca = torch.randn(4, 1, 5330, 1024)\n",
    "cacca.reshape(1, -1, 1024).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)flatten after backbone + linear decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stage5', 'stage12', 'stage18', 'stage24']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 4 features maps from the backbone\n",
    "depthv2.backbone.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21320, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_depth = depthv2.backbone(data)\n",
    "    out_depth = torch.stack(out_depth.feature_maps)\n",
    "    print(out_depth.reshape(1, 5330*4, 1024).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605184"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_stuff_dino = sum([p.numel() for p in model.encoder.patch_embed.parameters() if p.requires_grad]) + \\\n",
    "    sum([p.numel() for p in model.encoder.norm.parameters() if p.requires_grad])\n",
    "other_stuff_dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5458944"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.encoder.parameters() if p.requires_grad]) - transformer_dino - other_stuff_dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054016"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinov2_params = sum([p.numel() for p in model.encoder.parameters() if p.requires_grad])\n",
    "depthv2_params = sum([p.numel() for p in depthv2.backbone.parameters() if p.requires_grad])\n",
    "dinov2_params - depthv2_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DinoV2 patch_embed params:\n",
      "proj.weight True 602112\n",
      "proj.bias True 1024\n",
      "--------------------------------------------------\n",
      "DepthV2 patch_embed params:\n",
      "cls_token True 1024\n",
      "mask_token True 1024\n",
      "position_embeddings True 1402880\n",
      "patch_embeddings.projection.weight True 602112\n",
      "patch_embeddings.projection.bias True 1024\n",
      "--------------------------------------------------\n",
      "params difference: 1404928\n"
     ]
    }
   ],
   "source": [
    "print('DinoV2 patch_embed params:')\n",
    "dinov2_embed_params = 0\n",
    "for name, params in model.encoder.patch_embed.named_parameters():\n",
    "    print(name, params.requires_grad, params.numel())\n",
    "    dinov2_embed_params += params.numel()\n",
    "\n",
    "print('-'*50)\n",
    "print('DepthV2 patch_embed params:')\n",
    "depthv2_embeddings_params = 0\n",
    "for name, params in depthv2.backbone.embeddings.named_parameters():\n",
    "    print(name, params.requires_grad, params.numel())\n",
    "    depthv2_embeddings_params += params.numel()\n",
    "\n",
    "print('-'*50)\n",
    "print('params difference:', depthv2_embeddings_params - dinov2_embed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_dino = sum([p.numel() for p in model.encoder.blocks.parameters() if p.requires_grad])\n",
    "transformer_depthv2 = sum([p.numel() for p in depthv2.backbone.encoder.parameters() if p.requires_grad])\n",
    "transformer_dino == transformer_depthv2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-vfm-ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
